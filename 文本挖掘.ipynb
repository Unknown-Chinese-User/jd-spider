{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import\n",
    "import pyLDAvis.sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)#解决引入pyLDAvis的报错问题 https://github.com/bmabey/pyLDAvis/issues/196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-archives",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "goods_list=[line.strip() for line in open('goods_list.txt', encoding=\"utf-8\").readlines()]\n",
    "\n",
    "stopword = [line.strip() for line in open('stopword.txt', encoding=\"utf-8\").readlines()]\n",
    "stopword.append(' ')\n",
    "stopword.append('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-antibody",
   "metadata": {},
   "source": [
    "## 分词+去停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_sentence(sentence,stopwords):\n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    # loading stop word list\n",
    "    re = []\n",
    "    for word in sentence_seged:\n",
    "        if word not in stopwords:\n",
    "            re.append(word)\n",
    "    return re\n",
    "\n",
    "def seg_comment(goods):\n",
    "    f= open(goods+'/comment.txt','r')\n",
    "    comment=f.read()\n",
    "#     for i in range(1,999):\n",
    "#         comments+=f.readline()\n",
    "    f.close()\n",
    "    segs = seg_sentence(comment,stopword)\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "segs_dic={}\n",
    "for goods in goods_list:\n",
    "    segs_dic[goods]=seg_comment(goods)\n",
    "    \n",
    "for (k,v) in segs_dic.items():\n",
    "    f=open(k+'/segs.txt','w')\n",
    "    for seg in v:\n",
    "        f.write(seg+' ')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(segs_dic))\n",
    "i=0\n",
    "for (k,v) in segs_dic.items():\n",
    "    print(len(v))\n",
    "    i+=len(v)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (k,v) in segs_dic.items():\n",
    "#     f=open(k+'/segs.txt','a')\n",
    "#     for seg in v:\n",
    "#         f.write(seg+' ')\n",
    "#     f.close()\n",
    "# 仅在第一次运行时进行分词后文本的保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "segs_dic={}\n",
    "for goods in goods_list:\n",
    "    segs_dic[goods]=open(goods+'/segs.txt').readline().split(' ')\n",
    "# print(len(segs_dic))\n",
    "# for (k,v) in segs_dic.items():\n",
    "#     print(len(v))\n",
    "#     print(v[0:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-garlic",
   "metadata": {},
   "source": [
    "## 生成词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "font='/System/Library/Fonts/Hiragino Sans GB.ttc'\n",
    "background_img='background.jpg'\n",
    "mask = np.array(Image.open(background_img).convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segs_to_wc(goods,segs,font='Hiragino Sans GB.ttc',bg='background.jpg'):\n",
    "    mask = np.array(Image.open(bg).convert('RGB'))\n",
    "    \n",
    "    wc = wordcloud.WordCloud(\n",
    "                font_path=font,  # 设置字体格式\n",
    "                mask = mask,    # 设置背景图\n",
    "                max_words=400,     # 最多显示词数\n",
    "                max_font_size=80,  # 字体最大值\n",
    "                scale=16,           # 调整图片清晰度，值越大越清楚\n",
    "                background_color = \"white\"\n",
    "    )\n",
    "\n",
    "    frequency = collections.Counter(segs)\n",
    "    \n",
    "    wc.generate_from_frequencies(frequency)         # 从字典生成词云\n",
    "    image_colors = wordcloud.ImageColorGenerator(mask)  # 从背景图建立颜色方案\n",
    "    wc.recolor(color_func=image_colors)             # 将词云颜色设置为背景图方案\n",
    "\n",
    "    # plt.imshow(wc,interpolation='bilinear')\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.show()\n",
    "\n",
    "    wc.to_file(goods+'/wc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k,v) in segs_dic.items():\n",
    "    segs_to_wc(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-story",
   "metadata": {},
   "source": [
    "## 以下写主题分析与词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_top_words(model, feature_names, n_top_words,goods):\n",
    "    f=open(goods+'/topic.txt','w')\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        f.write(\"Topic #%d:\" % (topic_idx+1))\n",
    "        f.write('\\n')\n",
    "        f.write(\" \".join([feature_names[i]\n",
    "            for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "def topic_anaz(goods,segs):\n",
    "    n_features = 1000\n",
    "    tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                    max_features=n_features,\n",
    "                                    max_df = 0.5,\n",
    "                                    min_df = 10)\n",
    "    tf = tf_vectorizer.fit_transform(pd.array(segs))\n",
    "    \n",
    "    n_topics = 3\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics,\n",
    "                                    max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "    \n",
    "    lda.fit(tf)#运行时间很长\n",
    "    \n",
    "    n_top_words = 20\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    save_top_words(lda, tf_feature_names, n_top_words,goods)\n",
    "    \n",
    "    pyLDAvis.enable_notebook()\n",
    "    LDAvis_prepared=pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "    pyLDAvis.save_html(LDAvis_prepared,goods+'/topic.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_start=time.time()\n",
    "\n",
    "for (k,v) in segs_dic.items():\n",
    "    topic_anaz(k,v)\n",
    "    \n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
